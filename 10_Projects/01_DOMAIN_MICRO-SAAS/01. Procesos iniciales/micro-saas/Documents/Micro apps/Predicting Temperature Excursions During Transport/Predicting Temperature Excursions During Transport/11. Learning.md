##### Esta conversación continúa la anterior llamada ‘Use Case: Predicting Temperature Excursions During’. Retomemos desde el análisis de los datos de sensores

##### Ejecuta los DDL en tu SQL Server (artifacts/sql/…). Crea los stored procedures de upsert. Corre el Worker .NET (o un job SQL) que calcule y cargue los KPIs a diario. Levanta la API REST con los GET /api/kpis y /api/kpi-series. Conecta cualquier dashboard a la misma fuente (SQL o JSON).

##### ¿Quieres que te entregue:

##### Un repo ZIP con la solución .NET (Worker + API + scripts SQL + README)? La diapositiva también en SVG y un PowerPoint con esta y otra slide de “Ventajas competitivas”?}


¡Perfecto, Armando! Ya te preparé **todo el paquete** para continuar el caso “_Predicting Temperature Excursions During…_” desde el análisis de datos hasta el consumo en dashboards:

**Incluye:**

- Scripts SQL (DDL + SPs de upsert + KPI diario).
- Worker .NET que corre a diario (o bajo demanda) el cálculo de KPIs.
- API REST mínima con `GET /api/kpis` y `GET /api/kpi-series`.
- README con pasos de ejecución y conexión a dashboards.
- Una diapositiva en **SVG** y un **PowerPoint** con otra slide de “Ventajas competitivas”.

Descarga el repo ZIP aquí: [Repo ZIP: AHS.TempExcursions](blob:https://www.microsoft365.com/b5691134-aea8-48d1-913e-664258c28b39)

---

## Qué hice y cómo usarlo

### 1) DDL y SPs (artifacts/sql/…)

- **`001_ddl.sql`**: crea esquema `te` y tablas:
    - `te.Sensors`: catálogo con umbrales por sensor.
    - `te.SensorReadings`: lecturas crudas en UTC.
    - `te.KPI_Daily`: KPIs diarios por sensor (avg/min/max, puntos y minutos en excursión).
    - `te.KPI_SeriesCache`: cache opcional para series (buckets p.ej. 15 minutos).
- **`002_stored_procedures.sql`**:
    - `te.usp_UpsertSensor`: upsert por `ExternalId`.
    - `te.usp_UpsertSensorReading`: upsert lectura (mismo segundo = update).
    - `te.usp_ComputeDailyKPIs @UtcDate`: calcula KPIs del día (UTC) y hace `MERGE` a `KPI_Daily`.

> **Cómo ejecutarlo**
> 
> 1. Crear BD `TempExcursions`.
> 2. Ejecutar `001_ddl.sql` y luego `002_stored_procedures.sql`.

### 2) Upserts de datos crudos

Usa los SPs:

- `te.usp_UpsertSensor @ExternalId, @Location, @Asset, @MinThresholdC, @MaxThresholdC, @IsActive`
- `te.usp_UpsertSensorReading @ExternalId, @ReadingTimeUtc, @TemperatureC, @MetadataJson`

### 3) Cálculo y carga de KPIs (Worker .NET o Job SQL)

- **Worker** (`src/Worker`): .NET 8 + SqlClient + Dapper.
    - Ejecuta: `dotnet run --project src/Worker -- 2025-12-16` (fecha UTC opcional; por defecto: ayer).
    - Variable de entorno opcional: `TE_CONN` (connection string).
- **Job SQL** (alternativa): cron diario que llame\ `EXEC te.usp_ComputeDailyKPIs @UtcDate = CONVERT(date, GETUTCDATE());`

> Los **MinutesAbove/Below** se estiman con el `DATEDIFF` entre lecturas consecutivas (supone muestreo aproximadamente uniforme). Si tu sampling cambia, ajustamos el método (ej. integración por trapezoides).

### 4) API REST (GET /api/kpis y /api/kpi-series)

- **API** (`src/Api`): ASP.NET Core minimal API (.NET 8).
    - Arranque:
        
        cd src/Api
        
        dotnet run
        
    - Endpoints:
        - `GET /api/kpis?sensorId=1&date=2025-12-15` → filas de `te.KPI_Daily`.
        - `GET /api/kpi-series?sensorId=1&from=2025-12-01&to=2025-12-16` → buckets de `te.KPI_SeriesCache`.
    - Config: `appsettings.json` o variable `TE_CONN`.

### 5) Dashboards

Conecta **Power BI**, **Grafana** o **Excel** a:

- **SQL**: tablas `te.KPI_Daily` y `te.KPI_SeriesCache`.
- **JSON**: la API anterior (útil para portales o frontends).

---

## Artefactos de comunicación

- **SVG** (docs/slide_competitive_advantages.svg): arquitectura “Data → SQL → KPIs → API → Dashboards”.
- **PowerPoint** (docs/CompetitiveAdvantages.pptx):
    - Slide 1: resumen del pipeline y KPI list.
    - Slide 2: “Ventajas competitivas” (modularidad, auditabilidad, single source of truth, umbrales configurables, API mínima).

---

## Estructura del repo

```
AHS.TempExcursions/
├── README.md
├── artifacts/
│   └── sql/
│       ├── 001_ddl.sql
│       └── 002_stored_procedures.sql
├── src/
│   ├── Worker/
│   │   ├── Program.cs
│   │   └── Worker.csproj
│   └── Api/
│       ├── Program.cs
│       ├── Api.csproj
│       └── appsettings.json
└── docs/
    ├── slide_competitive_advantages.svg
    └── CompetitiveAdvantages.pptx
```

---

## Siguientes pasos sugeridos (rápidos)

1. **Provisiona** la BD y corre los scripts (producción/QA/dev).
2. **Carga sensores** (con umbrales realistas de excursión).
3. **Ingesta lecturas** con upsert (desde tus files/stream).
4. **Programa** el worker diario o el job SQL.
5. **Conecta** un dashboard (SQL directo o API JSON).

---

## Preguntas para afinar (sin frenar el avance)

- ¿La **frecuencia de muestreo** típica por sensor (p.ej., cada 1, 5 o 15 minutos)? Ajusto el cálculo de **minutes above/below** y los **buckets** de `KPI_SeriesCache`.
- ¿Quieres **índices adicionales** (por rango de fechas) o **particionamiento** por `KpiDate` para volumen alto?
- ¿Prefieres **autenticación** en la API (API key/JWT) desde el inicio?
- ¿Te genero un **script de carga masiva** (BULK INSERT) para lecturas históricas?

---

¿Te preparo también una versión con **Docker Compose** (SQL Server + API + Worker programado con `cron`) y un **dashboard de ejemplo** en Power BI/Grafana conectado a la API? Si te sirve, lo añado al repo y te entrego un ZIP actualizado en el siguiente turno.